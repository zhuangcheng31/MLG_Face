{
    "name": "DICGAN_Helen_SFT_remove_feature_loss_0429",
    "_comment": "68 face landmarks, connect HG over steps by feature average between each stacks",
    "mode": "sr_align_gan",
    "gpu_ids": [0,1] ,  //[0,1],
    "use_tb_logger": true,
    "scale": 8,
    "is_train": true,
    "rgb_range": 1,
    "save_image": true,
    "datasets": {
        "train": {
            "mode": "HRLandmark",
           "name": "HelenLandmarkTrain",
            "dataroot_HR": "/opt/data/private/helen/helen_all",//"D:\\DATASETS\\helen\\helen_all",   //"/home/jzy/datasets/SR_datasets/Helen/img_helen",
            "info_path": "../annotations/Helen_train_2K.pkl",//"/home/jzy/datasets/SR_datasets/Helen/train_info_list_68.pkl",
            "data_type": "img",
            "n_workers": 8,
            "batch_size": 4,
            "LR_size": 16,
            "HR_size": 128,
            "distort": [0.66, 1.33],
            "use_flip": true,
            "use_rot": true,
            "sigma": 1              // sigma of heatmap
        },
        "val": {
            "mode": "HRLandmark",
            "name": "HelenLandmarkVal",
            "dataroot_HR": "/opt/data/private/helen/helen_all",
            "info_path": "/opt/data/private/FSR/mlg_face/annotations/Helen_val.pkl",
            "data_type": "img",
            "LR_size": 16,
            "HR_size": 128,
            "sigma": 1
        }
    },
    "networks": {
        "which_model": "MLG",
        "num_features": 48,
        "in_channels": 3,
        "out_channels": 3,
        "num_steps": 4,
        "num_groups": 6,
        "detach_attention": false,   // whether detach attention map to stop gradient flow
        "hg_num_feature": 256,    // number of HourGlass features
        "hg_num_keypoints": 68,   // number of face keypoints
        "num_fusion_block": 7       // number of blocks in attention fusion module
    },
    "net_D": {
       "which_model_D": "LightCNN"
    },
    "net_F": {
      "which_model_F": "LightCNN"
    },
    "solver": {
        "type": "ADAM",
        "learning_rate_G": 0.0001,
        "weight_decay_G": 0,
        "learning_rate_D": 0.0001,
        "weight_decay_D": 0,
        "lr_scheme": "MultiStepLR",
        "lr_steps": [
            1e4, 2e4, 4e4
        ],
        "lr_gamma": 0.5,
        "manual_seed": 0,
        "save_freq": 4e3,
        "val_freq": 2e3, //2e3
        "niter": 1e5,  //1e5,      //100000 niters    2000
        "num_save_image": 10, //20,
        "log_full_step": true,  // whether to save&log SR images of all the intermediate steps.
        "pretrain": true, // pre-train mode: false (from scratch) | "resume" (resume from specific checkpoint) | true (finetune a new model based on a specific model)
        "pretrained_path":"/opt/data/private/FSR/mlg_face/experiments/DIC_in3f48_x8_debug_Helen_useColla_0331/epochs/step_0060000_ckp.pth", //"../models/LightCNN_feature.pth",//"/root/tmp/mlg_face/models/DICGAN_Helen.pth",//"../models/LightCNN_feature.pth",  //"extractor_pretrained_path": "../models/LightCNN_feature.pth", //'/root/tmp/mlg_face/models/DICGAN_Helen.pth', // path to pretrained model (if "pretrain" is not false)
        //"generator_pretrained_path": "/root/tmp/mlg_face/models/DICGAN_Helen.pth",
        "release_HG_grad_step": 2e3,  //2e6,
        "loss": {
            "pixel": {
                "loss_type": "l1",
                "weight": 2
            },
            "align": {
                "loss_type": "l2",
                "weight": 1e-1
            },
            "feature": {
              "loss_type": "l1",
              "weight": 0 //0.1
            },
            "GAN": {
              "loss_type": "GAN",
              "gan_type": "vanilla",
              "weight": 0.005
            }
        }
    },
    "logger": {
        "print_freq": 250
    },
    "path": {
      "root": "../"  //"/root/tmp/mlg_face/"
    }
}

